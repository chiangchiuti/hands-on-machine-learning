{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Volumes/MacUSB/Macbook/08 USB/OpenSource/pytorch-widedeep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/Volumes/ExFAT/dataset/adult.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 305 µs, sys: 0 ns, total: 305 µs\n",
      "Wall time: 311 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.columns = df.columns.map(lambda c: c.replace('-', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'educational_num',\n",
       "       'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
       "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
       "       'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wide features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Dict, List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cross_col_pairs: List[Tuple[str, str]]):\n",
    "        self.cross_col_pairs = cross_col_pairs\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        self.unique_columns_ = set()\n",
    "        for pair in self.cross_col_pairs:\n",
    "            self.unique_columns_.update(list(pair))\n",
    "        \n",
    "        self.crossed_colnamed_ = []\n",
    "        \n",
    "        for cols in self.cross_col_pairs:\n",
    "            cols = list(cols)\n",
    "            new_colname = \"_\".join(cols)\n",
    "            self.crossed_colnamed_.append(new_colname)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df: pd.DataFrame):\n",
    "            df_cross = df[self.unique_columns_].copy()\n",
    "            \n",
    "            for cols in self.cross_col_pairs:\n",
    "                cols = list(cols)\n",
    "                new_colname = \"_\".join(cols)\n",
    "                df_cross[new_colname] = df_cross[cols[0]] + \\\n",
    "                    '-' + df_cross[cols[1]]\n",
    "            return df_cross[self.crossed_colnamed_]\n",
    "           \n",
    "class WideFeaturesGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wide_cols: List[str], cross_col_pairs=None):\n",
    "        self.wide_cols = wide_cols\n",
    "        self.cross_col_pairs = cross_col_pairs\n",
    "\n",
    "        self.cross_feature_transformer = CrossFeatures(self.cross_col_pairs)\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        self.cross_feature_transformer_ = self.cross_feature_transformer.fit(df)\n",
    "        self.crossed_colnamed_ = self.cross_feature_transformer_.crossed_colnamed_\n",
    "\n",
    "        df_wide = self._generate_wide_features(df)\n",
    "        self.wide_columns_ = df_wide.columns.tolist()\n",
    "        self.feature_dict_ = self._generate_global_feature_dict(df_wide)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        \n",
    "        df_wide = self._generate_wide_features(df)\n",
    "        \n",
    "        encoded = np.zeros([len(df_wide), len(self.wide_columns_)], dtype=np.long)\n",
    "        \n",
    "        for i , col in enumerate(self.wide_columns_):\n",
    "            encoded[:, i] = df_wide[col].apply(lambda x: self.feature_dict_[col +  '_' + str(x)])\n",
    "        return encoded.astype('int64')\n",
    "        \n",
    "\n",
    "    def _generate_global_feature_dict(self, df: pd.DataFrame):\n",
    "        columns = df.columns.tolist()\n",
    "        all_col_value = []\n",
    "        for col in columns:\n",
    "            unique_value = [col + '_' + str(x) for x in df[col].unique()]\n",
    "            all_col_value.extend(unique_value)\n",
    "        return {v: i + 1 for i, v in enumerate(all_col_value)}\n",
    "\n",
    "\n",
    "    \n",
    "    def _generate_wide_features(self, df):\n",
    "        df_cross = self.cross_feature_transformer_.transform(df)\n",
    "        df_wide = pd.concat([df[self.wide_cols], df_cross], axis=1)\n",
    "        return df_wide    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = ['education', 'relationship','workclass','occupation','native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wideGenerator = WideFeaturesGenerator(wide_cols, crossed_cols)\n",
    "x_wide = wideGenerator.fit_transform(df)\n",
    "x_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  17,  23, ...,  89,  91, 316],\n",
       "       [  2,  18,  23, ...,  89,  92, 317],\n",
       "       [  3,  18,  24, ...,  89,  93, 318],\n",
       "       ...,\n",
       "       [  2,  20,  23, ...,  90, 103, 323],\n",
       "       [  2,  17,  23, ...,  89, 103, 323],\n",
       "       [  2,  21,  29, ...,  90, 115, 324]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_encode: List[str]):\n",
    "        self.columns_to_encode = columns_to_encode\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        df_ = df[self.columns_to_encode].copy()\n",
    "\n",
    "        for col in self.columns_to_encode:\n",
    "            df_[self.columns_to_encode] = df[self.columns_to_encode].astype(\n",
    "                'str')\n",
    "\n",
    "        unique_column_vals = {col: df_[col].unique()\n",
    "                              for col in self.columns_to_encode}\n",
    "\n",
    "        self.encoding_dict_ = dict()\n",
    "\n",
    "        for k, v in unique_column_vals.items():\n",
    "            self.encoding_dict_[k] = {val: idx for idx, val in enumerate(v)}\n",
    "            self.encoding_dict_[k]['unseen'] = len(self.encoding_dict_[k])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        try:\n",
    "            self.encoding_dict_\n",
    "        except AttributeError:\n",
    "            raise NotFittedError(\n",
    "                \"This LabelEncoder instance is not fitted yet. \"\n",
    "                \"Call 'fit' with appropriate arguments before using this LabelEncoder.\"\n",
    "            )\n",
    "        df_ = df.copy()\n",
    "        df_[self.columns_to_encode] = df_[self.columns_to_encode].astype('str')\n",
    "\n",
    "        for col, encoding_map in self.encoding_dict_.items():\n",
    "            original_value = [f for f in encoding_map.keys() if f != 'unseen']\n",
    "            df_[col] = np.where(df_[col].isin(\n",
    "                original_value), df_[col], 'unseen')\n",
    "            df_[col] = df_[col].apply(lambda x: encoding_map[x])\n",
    "        return df_\n",
    "\n",
    "\n",
    "class DeepFeaturesGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, embed_cols: List[str], continuous_cols: List[str]):\n",
    "        # category type\n",
    "        self.embed_cols = embed_cols\n",
    "        self.continuous_cols = continuous_cols \n",
    "\n",
    "        self.deep_cols = embed_cols + continuous_cols\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        label_encoder = LabelEncoder(self.embed_cols)\n",
    "        self.label_encoder_ = label_encoder.fit(df)\n",
    "        self.embed_cols_unique_labels_ = {col: len(v) for col, v in self.label_encoder_.encoding_dict_.items()}\n",
    "\n",
    "        df_continuous = df[self.continuous_cols].copy()\n",
    "        self.scalar_ = StandardScaler().fit(df_continuous.values.astype('float'))\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        df_embed = df[self.embed_cols].copy()\n",
    "        df_continuous = df[self.continuous_cols].copy()\n",
    "\n",
    "        df_embed = self.label_encoder_.transform(df_embed)\n",
    "        df_continuous[self.continuous_cols] = self.scalar_.transform(df_continuous.values.astype('float'))\n",
    "        df_deep = pd.concat([df_embed, df_continuous], axis=1)[self.deep_cols]\n",
    "        return df_deep.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embed_dim_mapping = {\n",
    "    'education': 10,\n",
    "    'relationship': 8,\n",
    "    'workclass': 10,\n",
    "    'occupation': 10,\n",
    "    'native_country': 10\n",
    "}\n",
    "category_cols = list(category_embed_dim_mapping.keys())\n",
    "continuous_cols = ['age', 'hours_per_week']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_generator = DeepFeaturesGenerator(category_cols, continuous_cols)\n",
    "df_deep = df[category_cols + continuous_cols].copy()\n",
    "x_deep = deep_generator.fit_transform(df)\n",
    "x_deep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_wide, x_deep, target):\n",
    "        assert(x_wide.shape[0] == x_deep.shape[0] == target.shape[0])\n",
    "        self.x_wide = x_wide\n",
    "        self.x_deep = x_deep\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_wide[index], self.x_deep[index], self.target[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <=50K\n",
       "1    <=50K\n",
       "2     >50K\n",
       "3     >50K\n",
       "4    <=50K\n",
       "Name: income, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'].iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 ms, sys: 518 µs, total: 2.55 ms\n",
      "Wall time: 2.16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_y = df['income'].values.copy()\n",
    "target_y = (target_y == '>50K').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48842, 8), (48842, 7), (48842,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_deep = x_deep\n",
    "x_wide = x_wide\n",
    "x_wide.shape, x_deep.shape, target_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WideDeepDataset(x_deep, x_wide, target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide(nn.Module):\n",
    "    def __init__(self, wide_dim: int, predict_dim: int=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(wide_dim + 1, predict_dim, padding_idx=0)\n",
    "        self.bias = nn.Parameter(torch.zeros(predict_dim))\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_normal_(self.linear.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.linear.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "\n",
    "        # X [b_size, num_of_wide_features]\n",
    "        return self.linear(X.long()).sum(dim=1) + self.bias # [b_size, predict_dim]\n",
    "\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    EMBEDDING_LAYER_PREFIX = \"emb_layer\"\n",
    "    DENSE_LAYER_PREFIX = \"dense_layer\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 columns_index: Dict[str, int],\n",
    "                 embed_cols_info: List[Tuple[str, int, int]],  # (col_name, label_size, embeding_dim)\n",
    "                 continuous_cols: List[str],\n",
    "                 hidden_layer_neural: List[int],\n",
    "                 hidden_layer_dropout: List[float],\n",
    "                 embed_col_dropout: float=0.0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.columns_index = columns_index\n",
    "        self.embed_cols_info = embed_cols_info\n",
    "        self.continuous_cols = continuous_cols\n",
    "\n",
    "        self.embed_layers = self._create_embed_layers(embed_cols_info)\n",
    "        self.embed_dropout_layer = nn.Dropout(embed_col_dropout)\n",
    "\n",
    "        self.hidden_layer_neural = self._update_hidden_layer_neural(\n",
    "            hidden_layer_neural)\n",
    "\n",
    "        self.dense_layer = self._create_dense_layer(hidden_layer_dropout)\n",
    "        self.output_dim = hidden_layer_neural[-1]\n",
    "\n",
    "    def _create_embed_layers(self, embed_cols_info: List[Tuple[str, int, int]]):\n",
    "        return nn.ModuleDict({self.EMBEDDING_LAYER_PREFIX + '_' + col_name.replace(\".\", '_'): nn.Embedding(num_label, dim) for col_name, num_label, dim in embed_cols_info})\n",
    "\n",
    "    def _create_dense_layer(self, hidden_layer_dropout):\n",
    "        dense_dequential = nn.Sequential()\n",
    "        for i in range(1, len(self.hidden_layer_neural)):\n",
    "            dense_dequential.add_module(\n",
    "                \"{}_{}\".format(self.DENSE_LAYER_PREFIX, i - 1),\n",
    "                self._create_dense_component(\n",
    "                    self.hidden_layer_neural[i-1], self.hidden_layer_neural[i], hidden_layer_dropout[i-1], True)\n",
    "            )\n",
    "        return dense_dequential\n",
    "\n",
    "    def _update_hidden_layer_neural(self, hidden_layer_neurals: List[int]):\n",
    "        embed_dim = sum([embed[2] for embed in self.embed_cols_info])\n",
    "        continuous_dim = len(self.continuous_cols)\n",
    "        return [embed_dim + continuous_dim] + hidden_layer_neurals\n",
    "\n",
    "    def _create_dense_component(self, input_dim: int, output_dim: int, dropout_ratio: float=0.0, batch_norm=False):\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(output_dim))\n",
    "        layers.append(nn.Dropout(dropout_ratio))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __get_embeding_layer(self, embed_col):\n",
    "        embed_col = self.EMBEDDING_LAYER_PREFIX + '_' + embed_col.replace('.', '_')\n",
    "        return self.embed_layers[embed_col]\n",
    "        \n",
    "    def forward(self, deep_input_x: torch.Tensor):\n",
    "        embed_x = [\n",
    "           self.__get_embeding_layer(col)(deep_input_x[:, self.columns_index[col]].long())\n",
    "            for col, _, _ in self.embed_cols_info\n",
    "        ]\n",
    "\n",
    "        embed_x = torch.cat(embed_x, 1)\n",
    "\n",
    "        continuous_cols_idx = [self.columns_index[col]\n",
    "                              for col in self.continuous_cols]\n",
    "        continuous_x = deep_input_x[:, continuous_cols_idx].float()\n",
    "\n",
    "        x = torch.cat([embed_x, continuous_x], dim=1)\n",
    "        return self.dense_layer(x)  # [b_size, hidden_layer_last_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, wide: nn.Module, deep: nn.Module):\n",
    "        super().__init__()\n",
    "        \n",
    "        deep = nn.Sequential(\n",
    "            deep,\n",
    "            nn.Linear(deep.output_dim, 1)\n",
    "        )\n",
    "        self.wide_deep = nn.ModuleDict({\n",
    "            \"wide\": wide,\n",
    "            \"deep\": deep\n",
    "        })\n",
    "\n",
    "    def forward(self, x_wide: torch.Tensor, x_deep: torch.Tensor):\n",
    "        wide_out = self.wide_deep['wide'](x_wide)  # [b_size, num_of_wide_feature, wide_predict_dim]\n",
    "        deep_out = self.wide_deep['deep'](x_deep)  # [b_size, num]\n",
    "        out = wide_out + deep_out\n",
    "        return out.view(-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, x_wide: torch.Tensor, x_deep: torch.Tensor, threshold: int=0.5):\n",
    "        logistic = self.predict_probs(x_wide, x_deep)\n",
    "        return (logistic > threshold).int()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_probs(self, x_wide: torch.Tensor, x_deep: torch.Tensor):\n",
    "        out = self.forward(x_wide, x_deep)\n",
    "        return torch.sigmoid(out.view(-1).float())\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTRL optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTRL(Optimizer):\n",
    "    def __init__(self, params, alpha=1.0, beta=1.0, l1=1.0, l2=1.0):\n",
    "        if not 0.0 < alpha:\n",
    "            raise ValueError(\"Invalid alpha parameter: {}\".format(alpha))\n",
    "        if not 0.0 < beta:\n",
    "            raise ValueError(\"Invalid beta parameter: {}\".format(beta))\n",
    "        if not 0.0 <= l1:\n",
    "            raise ValueError(\"Invalid l1 parameter: {}\".format(l1))\n",
    "        if not 0.0 <= l2:\n",
    "            raise ValueError(\"Invalid l2 parameter: {}\".format(l2))\n",
    "\n",
    "\n",
    "        defaults = dict(alpha=alpha, beta=beta, l1=l1, l2=l2)\n",
    "        super(FTRL, self).__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['z'] = torch.zeros_like(p.data)\n",
    "                    state['n'] = torch.zeros_like(p.data)\n",
    "               \n",
    "                # previous z and n\n",
    "                z, n = state['z'], state['n']\n",
    "                \n",
    "                \n",
    "                w = (group['l1'] * z.sign()) / (group['l2'] + (group['beta'] + n.sqrt()) / group['alpha'])\n",
    "                # update p weight by condition\n",
    "                p.data = torch.where(z.abs() < torch.tensor(group['l1'], dtype=torch.float), torch.zeros_like(w), w)\n",
    "\n",
    "                sigma = ((torch.pow(grad, 2) + n) - n.sqrt()) / group['alpha']\n",
    "                z.add_(grad - sigma * p.data)\n",
    "                n.add_(torch.pow(grad, 2))\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48842"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/Volumes/ExFAT/dataset/adult.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = df.columns.map(lambda c: c.replace('-', '_'))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'educational_num',\n",
       "       'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
       "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
       "       'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = ['education', 'relationship','workclass','occupation','native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n",
    "\n",
    "category_embed_dim_mapping = {\n",
    "    'education': 10,\n",
    "    'relationship': 8,\n",
    "    'workclass': 10,\n",
    "    'occupation': 10,\n",
    "    'native_country': 10\n",
    "}\n",
    "category_cols = list(category_embed_dim_mapping.keys())\n",
    "continuous_cols = ['age', 'hours_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 8)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wideGenerator = WideFeaturesGenerator(wide_cols, crossed_cols)\n",
    "x_wide = wideGenerator.fit_transform(df)\n",
    "x_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 7)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_generator = DeepFeaturesGenerator(category_cols, continuous_cols)\n",
    "df_deep = df[category_cols + continuous_cols].copy()\n",
    "x_deep = deep_generator.fit_transform(df)\n",
    "x_deep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = df['income'].values.copy()\n",
    "target_y = (target_y == '>50K').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48842, 8), (48842, 7), (48842,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_wide.shape, x_deep.shape, target_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39073, 8), (39073, 7), (39073,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_wide, test_x_wide, train_x_deep, test_x_deep, train_y, test_y = train_test_split(x_wide, x_deep, target_y, test_size=0.2)\n",
    "train_x_wide.shape, train_x_deep.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_wide = torch.from_numpy(test_x_wide)\n",
    "test_x_deep =  torch.from_numpy(test_x_deep)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cal_metrics(model, X, Y):\n",
    "    model.eval()\n",
    "    x_wide, x_deep = X\n",
    "\n",
    "    y_target = Y.detach().numpy()\n",
    "    y_prob = model.predict_probs(x_wide, x_deep).detach().numpy()\n",
    "    y_pre_label = (y_prob > 0.5).astype(int)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y_target, y_pre_label).ravel()\n",
    "        \n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_target, y_prob)\n",
    "\n",
    "    model.train()\n",
    "    return {'prec': precision, 'recall': recall, 'acc': acc, 'auc': auc}\n",
    "    \n",
    "def validation_step(model: nn.Module, X, Y, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_wide, x_deep = X\n",
    "        y_pre = model(x_wide, x_deep)\n",
    "        loss = loss_fn(y_pre.view(-1).float(), Y.float())  # mean loss\n",
    "    \n",
    "    model.train()\n",
    "    return loss.item(), cal_metrics(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = WideDeepDataset(train_x_wide, train_x_deep, train_y)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols_info = [(col, deep_generator.embed_cols_unique_labels_[col], embed_dim)for col, embed_dim in category_embed_dim_mapping.items()]\n",
    "deep_column_idx = {col: i for i, col in enumerate(deep_generator.deep_cols)}\n",
    "hidden_layers = [64, 32]\n",
    "drop_out = [0.2, 0.2]\n",
    "# out_dim = hidden_layers[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the whole network with AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(x_wide).shape[0], predict_dim=1)\n",
    "deep = Deep(deep_column_idx, embed_cols_info, continuous_cols, hidden_layers, drop_out, 0.2)\n",
    "wide_deep = WideDeep(wide, deep)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "general_optimizer = torch.optim.AdamW(wide_deep.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:06<00:00, 90.09it/s, metrics={'prec': 0.6518, 'recall': 0.55, 'acc': 0.8225, 'auc': 0.8645}, train_loss=0.407, valid_loss=tensor(0.3802)]\n",
      "100%|██████████| 611/611 [00:06<00:00, 89.99it/s, metrics={'prec': 0.6757, 'recall': 0.5363, 'acc': 0.8279, 'auc': 0.8752}, train_loss=0.392, valid_loss=tensor(0.3654)]\n",
      "100%|██████████| 611/611 [00:07<00:00, 81.90it/s, metrics={'prec': 0.6861, 'recall': 0.5354, 'acc': 0.8307, 'auc': 0.8789}, train_loss=0.378, valid_loss=tensor(0.3601)]\n",
      "100%|██████████| 611/611 [00:07<00:00, 85.39it/s, metrics={'prec': 0.6931, 'recall': 0.5358, 'acc': 0.8326, 'auc': 0.882}, train_loss=0.376, valid_loss=tensor(0.3560)]\n",
      "100%|██████████| 611/611 [00:07<00:00, 79.44it/s, metrics={'prec': 0.6941, 'recall': 0.5431, 'acc': 0.8339, 'auc': 0.8836}, train_loss=0.374, valid_loss=tensor(0.3536)]\n",
      "100%|██████████| 611/611 [00:09<00:00, 62.39it/s, metrics={'prec': 0.6933, 'recall': 0.5363, 'acc': 0.8327, 'auc': 0.8854}, train_loss=0.369, valid_loss=tensor(0.3513)]\n",
      "100%|██████████| 611/611 [00:08<00:00, 72.59it/s, metrics={'prec': 0.6884, 'recall': 0.5526, 'acc': 0.8336, 'auc': 0.8859}, train_loss=0.366, valid_loss=tensor(0.3503)]\n",
      "100%|██████████| 611/611 [00:08<00:00, 71.19it/s, metrics={'prec': 0.6989, 'recall': 0.5337, 'acc': 0.8339, 'auc': 0.8858}, train_loss=0.362, valid_loss=tensor(0.3504)]\n",
      "100%|██████████| 611/611 [00:07<00:00, 86.74it/s, metrics={'prec': 0.7046, 'recall': 0.5393, 'acc': 0.8361, 'auc': 0.886}, train_loss=0.365, valid_loss=tensor(0.3498)]\n",
      "100%|██████████| 611/611 [00:07<00:00, 81.68it/s, metrics={'prec': 0.6978, 'recall': 0.541, 'acc': 0.8346, 'auc': 0.8869}, train_loss=0.365, valid_loss=tensor(0.3491)]\n"
     ]
    }
   ],
   "source": [
    "log_interval = 50\n",
    "epoch = 10\n",
    "for epoch_i in range(epoch):\n",
    "    total_loss = 0\n",
    "   \n",
    "    tk0 = tqdm.tqdm(train_data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (x_w, x_d, target_y) in enumerate(tk0):\n",
    "        wide_deep.train()\n",
    "        y = wide_deep(x_w, x_d)\n",
    "        loss = criterion(y.view(-1), target_y.float())\n",
    "        \n",
    "        wide_deep.zero_grad()\n",
    "        loss.backward()\n",
    "        general_optimizer.step()\n",
    "#         wide_optimizer.step()\n",
    "#         deep_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % log_interval == 0:\n",
    "            valid_loss, score = validation_step(wide_deep, (test_x_wide, test_x_deep), test_y, criterion)\n",
    "            tk0.set_postfix(train_loss=total_loss/log_interval, valid_loss=valid_loss, metrics={k: np.round(v, 4) for k, v in score.items()})\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8340669464633023,\n",
       " 'auc': 0.887210434803874,\n",
       " 'prec': 0.6927252985884907,\n",
       " 'recall': 0.5474045474045474}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_metrics(wide_deep, (test_x_wide, test_x_deep), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train wide deep with different optmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(x_wide).shape[0], predict_dim=1)\n",
    "deep = Deep(deep_column_idx, embed_cols_info, continuous_cols, hidden_layers, drop_out, 0.2)\n",
    "wide_deep = WideDeep(wide, deep)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "wide_optimizer = FTRL(wide_deep.wide_deep['wide'].parameters(), alpha=1, beta=1, l1=0.5, l2=0)\n",
    "deep_optimizer = torch.optim.Adagrad(wide_deep.wide_deep['deep'].parameters(), lr=0.01, lr_decay=0.9, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:08<00:00, 75.23it/s, metrics={'prec': 0.2759, 'recall': 0.9863, 'acc': 0.3791, 'auc': 0.7567}, train_loss=1.03, valid_loss=0.988]\n",
      "100%|██████████| 611/611 [00:07<00:00, 76.66it/s, metrics={'prec': 0.3035, 'recall': 0.9266, 'acc': 0.4751, 'auc': 0.7406}, train_loss=0.911, valid_loss=0.863]\n",
      "100%|██████████| 611/611 [00:07<00:00, 84.81it/s, metrics={'prec': 0.3061, 'recall': 0.9116, 'acc': 0.4857, 'auc': 0.7319}, train_loss=0.862, valid_loss=0.848]\n",
      "100%|██████████| 611/611 [00:07<00:00, 82.06it/s, metrics={'prec': 0.3263, 'recall': 0.9039, 'acc': 0.5317, 'auc': 0.7469}, train_loss=0.841, valid_loss=0.794]\n",
      "100%|██████████| 611/611 [00:05<00:00, 102.82it/s, metrics={'prec': 0.3303, 'recall': 0.8567, 'acc': 0.5513, 'auc': 0.7399}, train_loss=0.817, valid_loss=0.757]\n",
      "100%|██████████| 611/611 [00:06<00:00, 95.50it/s, metrics={'prec': 0.3209, 'recall': 0.9048, 'acc': 0.5203, 'auc': 0.745}, train_loss=0.798, valid_loss=0.799]\n",
      "100%|██████████| 611/611 [00:06<00:00, 100.12it/s, metrics={'prec': 0.323, 'recall': 0.8666, 'acc': 0.5349, 'auc': 0.7392}, train_loss=0.786, valid_loss=0.781]\n",
      "100%|██████████| 611/611 [00:06<00:00, 96.62it/s, metrics={'prec': 0.3265, 'recall': 0.8644, 'acc': 0.5421, 'auc': 0.7476}, train_loss=0.775, valid_loss=0.762]\n",
      "100%|██████████| 611/611 [00:06<00:00, 100.26it/s, metrics={'prec': 0.3366, 'recall': 0.8722, 'acc': 0.5593, 'auc': 0.7474}, train_loss=0.763, valid_loss=0.751]\n",
      " 71%|███████   | 434/611 [00:04<00:01, 104.07it/s, metrics={'prec': 0.3496, 'recall': 0.8194, 'acc': 0.5932, 'auc': 0.7517}, train_loss=0.75, valid_loss=0.714]"
     ]
    }
   ],
   "source": [
    "\n",
    "log_interval = 100\n",
    "for epoch_i in range(20):\n",
    "    total_loss = 0\n",
    "   \n",
    "    tk0 = tqdm.tqdm(train_data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (x_w, x_d, target_y) in enumerate(tk0):\n",
    "        wide_deep.train()\n",
    "        y = wide_deep(x_w, x_d)\n",
    "        loss = criterion(y.view(-1), target_y.float())\n",
    "        \n",
    "        wide_deep.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        wide_optimizer.step()\n",
    "        deep_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % log_interval == 0:\n",
    "            valid_loss, score = validation_step(wide_deep, (test_x_wide, test_x_deep), test_y, criterion)\n",
    "            tk0.set_postfix(train_loss=total_loss/log_interval, valid_loss=valid_loss, metrics={k: np.round(v, 4) for k, v in score.items()})\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_metrics(wide_deep, (test_x_wide, test_x_deep), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_deep.wide_deep['wide'].linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = wide_deep.wide_deep['wide'].linear.weight.view(-1)\n",
    "weight.nonzero().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
