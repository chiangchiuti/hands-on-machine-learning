{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bible-kjv.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gutenberg.sents(gutenberg.fileids()[3])\n",
    "pattern = re.compile(\"[A-Za-z]+\")\n",
    "stop_w =  set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for sent in samples:\n",
    "    sent = [w.lower() for w in sent]\n",
    "    sent = [w for w in sent if w not in stop_w]\n",
    "    sent = [w.replace('\\n', ' ') for w in sent]\n",
    "    sent = [w for w in sent if pattern.fullmatch(w)]\n",
    "    if len(sent) > 5:\n",
    "        corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_dist = FreqDist()\n",
    "for sent in corpus:\n",
    "    fre_dist.update(sent)\n",
    "fre_dist = {k : v for k, v in fre_dist.items() if v > 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(fre_dist)\n",
    "idx_to_word = {idx: word for idx,  word in enumerate(fre_dist.keys())}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert word to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indexed = [[word_to_idx[word] for word in sent if word in word_to_idx]for sent in corpus]\n",
    "corpus_indexed = [sent for sent in corpus_indexed if len(sent) > 5]\n",
    "fre_dist_indexed = {word_to_idx[w]: f for w, f in fre_dist.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, is_leaf, value=None, fre=0, left=None, right=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = value  # the node's index in huffman tree\n",
    "        self.fre = fre  # word frequency in corpus\n",
    "        self.code = []  # huffman code\n",
    "        self.code_len = 0  # lenght of code\n",
    "        self.node_path = []  # the path from root node to this node\n",
    "        self.left = left  # left child\n",
    "        self.right = right  # right child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way builing huffman tree refer to c's original implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self, fre_dict):\n",
    "        self.root = None\n",
    "        freq_dict = sorted(fre_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "        self.vocab_size = len(freq_dict)\n",
    "        self.node_dict = {}\n",
    "        self._build_tree(freq_dict)\n",
    "    \n",
    "    def _build_tree(self, freq_dict):\n",
    "        '''\n",
    "            freq_dict is in decent order\n",
    "            node_list: two part: [leaf node :: internal node]\n",
    "                leaf node is sorting by frequency in decent order; \n",
    "        '''\n",
    "    \n",
    "        node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict]  # create leaf node\n",
    "        node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(self.vocab_size)]  # create non-leaf node\n",
    "\n",
    "        parentNode = [0] * (self.vocab_size * 2)  # only 2 * vocab_size - 2 be used\n",
    "        binary = [0] * (self.vocab_size * 2)  # recording turning left or turning right\n",
    "        \n",
    "        '''\n",
    "          pos1 points to currently processing leaf node at left side of node_list\n",
    "          pos2 points to currently processing non-leaf node at right side of node_list\n",
    "        '''\n",
    "\n",
    "        pos1 = self.vocab_size - 1\n",
    "        pos2 = self.vocab_size\n",
    "        \n",
    "        '''\n",
    "            each iteration picks two node from node_list\n",
    "            the first pick assigns to min1i\n",
    "            the second pick assigns to min2i \n",
    "            \n",
    "            min2i's frequency is always larger than min1i\n",
    "        '''\n",
    "        min1i = 0\n",
    "        min2i = 0\n",
    "        '''\n",
    "            the main process of building huffman tree\n",
    "        '''\n",
    "        for a in range(self.vocab_size - 1):\n",
    "            '''\n",
    "                first pick assigns to min1i\n",
    "            '''\n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min1i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min1i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min1i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            '''\n",
    "               second pick assigns to min2i \n",
    "            '''\n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min2i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min2i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min2i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            ''' fill information of non leaf node '''\n",
    "            node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre\n",
    "            node_list[self.vocab_size + a].left = node_list[min1i]\n",
    "            node_list[self.vocab_size + a].right = node_list[min2i]\n",
    "            \n",
    "            '''\n",
    "                the parent node always is non leaf node\n",
    "                assigen lead child (min2i) and right child (min1i) to parent node\n",
    "            '''\n",
    "            parentNode[min1i] = self.vocab_size + a  # max index = 2 * vocab_size - 2\n",
    "            parentNode[min2i] = self.vocab_size + a\n",
    "            binary[min2i] = 1\n",
    "        \n",
    "        '''generate huffman code of each leaf node '''\n",
    "        for a in range(self.vocab_size):\n",
    "            b = a\n",
    "            i = 0\n",
    "            code = []\n",
    "            point = []\n",
    "\n",
    "            '''\n",
    "\n",
    "                backtrace path from current node until root node. (bottom up)\n",
    "                'root node index' in node_list is  2 * vocab_size - 2 \n",
    "            '''\n",
    "            while b != self.vocab_size * 2 - 2:\n",
    "                code.append(binary[b])  \n",
    "                b = parentNode[b]\n",
    "                # point recording the path index from leaf node to root, the length of point is less 1 than the length of code\n",
    "                point.append(b)\n",
    "            \n",
    "            '''\n",
    "                huffman code should be top down, so we reverse it.\n",
    "            '''\n",
    "            node_list[a].code_len = len(code)\n",
    "            node_list[a].code = list(reversed(code))\n",
    "            \n",
    "\n",
    "            '''\n",
    "                1. Recording the path from root to leaf node (top down). \n",
    "                \n",
    "                2.The actual index value should be shifted by self.vocab_size,\n",
    "                  because we need the index starting from zero to mapping non-leaf node\n",
    "                \n",
    "                3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1.\n",
    "                  The index of BST root node in node_list is 2 * vocab_size - 2,\n",
    "                  and we shift vocab_size to get the actual index of root node: vocab_size - 2\n",
    "            '''\n",
    "            node_list[a].node_path = list(reversed([p - self.vocab_size for p in point]))\n",
    "            \n",
    "            self.node_dict[node_list[a].value] = node_list[a]\n",
    "            \n",
    "        self.root = node_list[2 * vocab_size - 2]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW + HS\n",
    "$$\n",
    "-\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})}= - \\sum^{L(w)-1}_{l=1}  \\log\\sigma( [ \\cdot ] h^\\top \\text{v}^{'}_l)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, windows_size=5, sentence_length_threshold=5):\n",
    "        self.windows_size = windows_size\n",
    "        self.sentence_length_threshold = sentence_length_threshold\n",
    "        self.contexts, self.centers = self._generate_pairs(corpus, windows_size)\n",
    "        \n",
    "    def _generate_pairs(self, corpus, windows_size):\n",
    "        contexts = []\n",
    "        centers = []\n",
    "        \n",
    "        for sent in corpus:\n",
    "            if len(sent) < self.sentence_length_threshold:\n",
    "                continue\n",
    "            \n",
    "            for center_word_pos in range(len(sent)):\n",
    "                context = []\n",
    "                for w in range(-windows_size, windows_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    if(0 <= context_word_pos < len(sent) and context_word_pos != center_word_pos):\n",
    "                        context.append(sent[context_word_pos])\n",
    "                if(len(context) == 2 * self.windows_size):\n",
    "                    contexts.append(context)\n",
    "                    centers.append(sent[center_word_pos])\n",
    "        return contexts, centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return np.array(self.contexts[index]), np.array([self.centers[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, freq_dict):\n",
    "        super().__init__()\n",
    "        ## in w2v c implement, syn1 initial with all zero\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.syn1 = nn.Embedding(\n",
    "            num_embeddings=vocab_size + 1,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab_size\n",
    "            \n",
    "        )\n",
    "        torch.nn.init.constant_(self.syn1.weight.data, val=0)\n",
    "        self.huffman_tree = HuffmanTree(freq_dict)\n",
    "\n",
    "    def forward(self, neu1, target):\n",
    "        # neu1: [b_size, embedding_dim]\n",
    "        # target: [b_size, 1]\n",
    "        \n",
    "        # turns:[b_size, max_code_len_in_batch]\n",
    "        # paths: [b_size, max_code_len_in_batch]\n",
    "        turns, paths = self._get_turns_and_paths(target)\n",
    "        paths_emb = self.syn1(paths) # [b_size, max_code_len_in_batch, embedding_dim]\n",
    "\n",
    "        loss = -F.logsigmoid(\n",
    "            (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def _get_turns_and_paths(self, target):\n",
    "        turns = []  # turn right(1) or turn left(-1) in huffman tree\n",
    "        paths = []\n",
    "        max_len = 0\n",
    "        ''' we have batch of center words ... '''\n",
    "        for n in target:\n",
    "            n = n.item()\n",
    "            node = self.huffman_tree.node_dict[n]\n",
    "            \n",
    "            code = target.new_tensor(node.code).int()  # in code, left node is 0; right node is 1\n",
    "            turn = torch.where(code == 1, code, -torch.ones_like(code)) # 1 -> 1;  0 -> -1\n",
    "            \n",
    "            turns.append(turn)\n",
    "            '''node_path records the index from root to leaf node in huffman tree'''\n",
    "            paths.append(target.new_tensor(node.node_path))\n",
    "            \n",
    "            if node.code_len > max_len:\n",
    "                max_len = node.code_len\n",
    "        \n",
    "        '''Because each word may has different code length, we should pad them to equal length'''\n",
    "        turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns] \n",
    "        paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths]\n",
    "        return torch.stack(turns).int(), torch.stack(paths).long()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWHierarchicalSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, fre_dict):\n",
    "        super().__init__()\n",
    "        self.syn0 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict)\n",
    "\n",
    "    \n",
    "    def forward(self, context, target):\n",
    "        # context: [b_size, 2 * window_size]\n",
    "        # target: [b_size]\n",
    "        neu1 = self.syn0(context.long()).mean(dim=1)  # [b_size, embedding_dim]\n",
    "        loss = self.hs(neu1, target.long())\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = CBOWDataset(corpus_indexed)\n",
    "data_loader = DataLoader(data_set, batch_size=100, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "net = CBOWHierarchicalSoftmax(vocab_size, embedding_dim, fre_dist_indexed)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001,  weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1147/1147 [00:32<00:00, 35.12it/s, loss=10.9]\n",
      "100%|██████████| 1147/1147 [00:29<00:00, 38.94it/s, loss=10.7]\n",
      "100%|██████████| 1147/1147 [00:30<00:00, 37.59it/s, loss=10.6]\n",
      "100%|██████████| 1147/1147 [00:22<00:00, 50.96it/s, loss=10.5]\n",
      "100%|██████████| 1147/1147 [00:29<00:00, 39.32it/s, loss=10.4]\n",
      "100%|██████████| 1147/1147 [00:26<00:00, 43.69it/s, loss=10.3]\n",
      "100%|██████████| 1147/1147 [00:23<00:00, 47.86it/s, loss=10.2]\n",
      "100%|██████████| 1147/1147 [00:26<00:00, 43.45it/s, loss=10.1]\n",
      "100%|██████████| 1147/1147 [00:27<00:00, 41.68it/s, loss=10.1]\n",
      "100%|██████████| 1147/1147 [00:28<00:00, 40.07it/s, loss=10]\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100\n",
    "for epoch_i in range(10):\n",
    "    total_loss = 0\n",
    "    net.train()\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (context, center) in enumerate(tk0):\n",
    "\n",
    "        loss = net(context, center)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if(i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss = total_loss/log_interval)\n",
    "            total_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiangchiuti/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "w2v_embedding = net.syn0.weight.data\n",
    "w2v_embedding = w2v_embedding.numpy()\n",
    "l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)\n",
    "w2v_embedding = w2v_embedding / l2norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarity:\n",
    "    def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict):\n",
    "        self.word_embedding = word_embedding # normed already\n",
    "        self.idx_to_word_dict = idx_to_word_dict\n",
    "        self.word_to_idx_dict = word_to_idx_dict\n",
    "        \n",
    "    def get_synonym(self, word, topK=10):\n",
    "        idx = self.word_to_idx_dict[word]\n",
    "        embed = self.word_embedding[idx]\n",
    "        \n",
    "        cos_similairty = w2v_embedding @ embed\n",
    "        \n",
    "        topK_index = np.argsort(-cos_similairty)[:topK]\n",
    "        pairs = []\n",
    "        for i in topK_index:\n",
    "            w = self.idx_to_word_dict[i]\n",
    "            pairs.append((w, cos_similairty[i]))\n",
    "        return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christ', 1.0),\n",
       " ('hope', 0.78780156),\n",
       " ('gospel', 0.7656436),\n",
       " ('jesus', 0.74575657),\n",
       " ('faith', 0.7190881),\n",
       " ('godliness', 0.7005944),\n",
       " ('offences', 0.70045626),\n",
       " ('grace', 0.6946964),\n",
       " ('dear', 0.666232),\n",
       " ('willing', 0.66131693)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)\n",
    "cosinSim.get_synonym('christ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('god', 1.0),\n",
       " ('saviour', 0.53627664),\n",
       " ('remember', 0.51367503),\n",
       " ('sure', 0.4997003),\n",
       " ('hope', 0.47002873),\n",
       " ('purpose', 0.46906227),\n",
       " ('praise', 0.45354468),\n",
       " ('thanks', 0.4486973),\n",
       " ('doubtless', 0.44689322),\n",
       " ('formed', 0.44300675)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim.get_synonym('god')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jesus', 0.9999999),\n",
       " ('gospel', 0.8051339),\n",
       " ('grace', 0.75879383),\n",
       " ('church', 0.7542972),\n",
       " ('christ', 0.74575657),\n",
       " ('manifest', 0.7415799),\n",
       " ('believed', 0.7215627),\n",
       " ('faith', 0.7198993),\n",
       " ('godliness', 0.7091305),\n",
       " ('john', 0.7015951)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim.get_synonym('jesus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
