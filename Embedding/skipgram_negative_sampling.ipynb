{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random, math\n",
    "from collections import OrderedDict, Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bible-kjv.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gutenberg.sents(gutenberg.fileids()[3])\n",
    "pattern = re.compile(\"[A-Za-z]+\")\n",
    "stop_w =  set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for sent in samples:\n",
    "    sent = [w.lower() for w in sent]\n",
    "    sent = [w for w in sent if w not in stop_w]\n",
    "    sent = [w.replace('\\n', ' ') for w in sent]\n",
    "    sent = [w for w in sent if pattern.fullmatch(w)]\n",
    "    if len(sent) > 5:\n",
    "        corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_dist = FreqDist()\n",
    "for sent in corpus:\n",
    "    fre_dist.update(sent)\n",
    "fre_dist = {k : v for k, v in fre_dist.items() if v > 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(fre_dist)\n",
    "idx_to_word = {idx: word for idx,  word in enumerate(fre_dist.keys())}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indexed = [[word_to_idx[word] for word in sent if word in word_to_idx]for sent in corpus]\n",
    "corpus_indexed = [sent for sent in corpus_indexed if len(sent) > 5]\n",
    "fre_dist_indexed = {word_to_idx[w]: f for w, f in fre_dist.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    def __init__(self, corpus, sample_ratio=0.75):\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.sample_table =  self.__build_sample_table(corpus)\n",
    "        self.table_size = len(self.sample_table)\n",
    "        \n",
    "    def __build_sample_table(self, corpus):\n",
    "        counter = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "        words = np.array(list(counter.keys()))\n",
    "        probs = np.power(np.array(list(counter.values())), self.sample_ratio)\n",
    "        normalizing_factor = probs.sum()\n",
    "        probs = np.divide(probs, normalizing_factor)\n",
    "        \n",
    "        sample_table = []\n",
    "\n",
    "        table_size = 1e8\n",
    "        word_share_list = np.round(probs * table_size)\n",
    "        '''\n",
    "         the higher prob, the more shares in  sample_table\n",
    "        '''\n",
    "        for w_idx, w_fre in enumerate(word_share_list):\n",
    "            sample_table += [words[w_idx]] * int(w_fre)\n",
    "\n",
    "#         sample_table = np.array(sample_table) // too slow\n",
    "        return sample_table\n",
    "    \n",
    "    def generate(self, sample_size=6):\n",
    "\n",
    "        negatvie_samples = [self.sample_table[idx] for idx in np.random.randint(0, self.table_size, sample_size)]\n",
    "        return np.array(negatvie_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramWithNGEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, window_size=5, sentence_length_threshold=5, negative_sample_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.sentence_length_threshold = sentence_length_threshold\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        \n",
    "        self.corpus = self.__subsampling_frequenct_words(corpus)\n",
    "        self.pairs = self.__generate_pairs(self.corpus, window_size)\n",
    "        self.negative_sampler = NegativeSampler(self.corpus)\n",
    "        \n",
    "    def __sub_sample(self, x, alpha=3):\n",
    "        pow_ = math.pow(10, alpha)\n",
    "        s = math.sqrt(x * pow_)\n",
    "        return (s + 1) / (x * pow_)\n",
    "    \n",
    "    def __subsampling_frequenct_words(self, corpus):\n",
    "        counter = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "        sum_word_count = sum(list(counter.values()))\n",
    "        \n",
    "        word_ratio ={w: count / sum_word_count  for w, count in counter.items()}\n",
    "        \n",
    "        word_subsample_frequency = {k: self.__sub_sample(v) for k, v in word_ratio.items()}\n",
    "        \n",
    "        filtered_corpus = [] \n",
    "        for sent in corpus:\n",
    "            filtered_sent = []\n",
    "            for w in sent:\n",
    "                if random.random() < word_subsample_frequency[w]:\n",
    "                      filtered_sent.append(w)\n",
    "            filtered_corpus.append(filtered_sent)\n",
    "        return filtered_corpus\n",
    "    \n",
    "    \n",
    "        \n",
    "    def __generate_pairs(self, corpus, windows_size):     \n",
    "        pairs = []\n",
    "        for sentence in corpus:\n",
    "            if len(sentence) < self.sentence_length_threshold:\n",
    "                continue\n",
    "\n",
    "            for center_word_pos in range(len(sentence)):\n",
    "                for shift in range(-windows_size, windows_size + 1):\n",
    "                    context_word_pos = center_word_pos + shift\n",
    "                    \n",
    "                    if (0 <= context_word_pos < len(sentence)) and context_word_pos != center_word_pos:\n",
    "                        pairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "        return pairs  # [(centerword, a_context_word)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    '''\n",
    "        @return:  1 center_w, 1 context_w, n negative sample words\n",
    "    '''\n",
    "    def __getitem__(self, index):\n",
    "        center_w, context_w = self.pairs[index]\n",
    "\n",
    "        return np.array([center_w]), np.array([context_w]), self.negative_sampler.generate(self.negative_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram with negative sampling\n",
    "$$\n",
    "\\mathcal{L}_\\theta = - [ \\log \\sigma(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,j}}) +  \\sum^M_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{\\tilde{w}_i})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNEG(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.syn0 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K|\n",
    "        self.neg_syn1 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K|\n",
    "        torch.nn.init.constant_(self.neg_syn1.weight.data, val=0)\n",
    "        \n",
    "    def forward(self, center: torch.Tensor, context: torch.Tensor, negative_samples: torch.Tensor):\n",
    "        # center : [b_size, 1]\n",
    "        # context: [b_size, 1]\n",
    "        # negative_sample: [b_size, negative_sample_num]\n",
    "        embd_center = self.syn0(center)  # [b_size, 1, embedding_dim]\n",
    "        embd_context = self.neg_syn1(context) # [b_size, 1, embedding_dim]\n",
    "        embd_negative_sample = self.neg_syn1(negative_samples) # [b_size, negative_sample_num, embedding_dim]\n",
    "        \n",
    "        prod_p =  (embd_center * embd_context).sum(dim=1).squeeze()  # [b_size]\n",
    "        loss_p =  F.logsigmoid(prod_p).mean() # 1\n",
    "        \n",
    "        \n",
    "        prod_n = (embd_center * embd_negative_sample).sum(dim=2) # [b_size, negative_sample_num]\n",
    "        loss_n = F.logsigmoid(-prod_n).sum(dim=1).mean() # 1\n",
    "        return -(loss_p + loss_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = SkipGramNEG(vocab_size, EMBEDDING_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramWithNGEDataset(corpus_indexed)\n",
    "data_loader = DataLoader(dataset, batch_size=500, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4205/4205 [04:36<00:00, 15.22it/s, loss=0.93]\n",
      "100%|██████████| 4205/4205 [03:51<00:00, 18.18it/s, loss=0.757]\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100\n",
    "for epoch_i in range(2):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (center, context, neg_samples) in enumerate(tk0):\n",
    "        loss = model(center, context, neg_samples)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if(i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss = total_loss/log_interval)\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = model.syn0.weight.data\n",
    "neg_syn1 = model.neg_syn1.weight.data\n",
    "\n",
    "w2v_embedding = (syn0 + neg_syn1) / 2\n",
    "w2v_embedding = w2v_embedding.numpy()\n",
    "l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)\n",
    "w2v_embedding = w2v_embedding / l2norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "class NNeighbors:\n",
    "    def __init__(self, NN_model, word_embedding, idx_to_word_dict, word_to_idx_dict):\n",
    "        self.NN_model = NN_model\n",
    "        self.word_embedding = word_embedding\n",
    "        self.idx_to_word_dict = idx_to_word_dict\n",
    "        self.word_to_idx_dict = word_to_idx_dict\n",
    "    \n",
    "    def get_neighbors(self, word):\n",
    "        idx = self.word_to_idx_dict[word]\n",
    "        embedding = self.word_embedding[[idx]]\n",
    "        dists, neighbors = self.NN_model.kneighbors(embedding)\n",
    "        dists, neighbors = dists[0], neighbors[0]\n",
    "        \n",
    "        pairs = OrderedDict()\n",
    "        for n_idx, d in zip(neighbors, dists):\n",
    "            pairs[self.idx_to_word_dict[n_idx]] = d\n",
    "        return pairs\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor = NearestNeighbors(n_neighbors=10, algorithm='auto', p=2).fit(w2v_embedding)\n",
    "nn_neighbors = NNeighbors(neighbor, w2v_embedding, idx_to_word, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('jesus', 0.0),\n",
       "             ('christ', 0.7522091979029454),\n",
       "             ('gospel', 0.9632666194513442),\n",
       "             ('peter', 1.1310211737966784),\n",
       "             ('disciples', 1.1436218856215599),\n",
       "             ('church', 1.2036696221080758),\n",
       "             ('passed', 1.2268319584910587),\n",
       "             ('noise', 1.2347588831506333),\n",
       "             ('preach', 1.2372968309935133),\n",
       "             ('send', 1.2379122748838614)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_neighbors.get_neighbors('jesus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarity:\n",
    "    def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict):\n",
    "        self.word_embedding = word_embedding # normed already\n",
    "        self.idx_to_word_dict = idx_to_word_dict\n",
    "        self.word_to_idx_dict = word_to_idx_dict\n",
    "        \n",
    "    def get_synonym(self, word, topK=10):\n",
    "        idx = self.word_to_idx_dict[word]\n",
    "        embed = self.word_embedding[idx]\n",
    "        \n",
    "        cos_similairty = w2v_embedding @ embed\n",
    "        \n",
    "        topK_index = np.argsort(-cos_similairty)[:topK]\n",
    "        pairs = []\n",
    "        for i in topK_index:\n",
    "            w = self.idx_to_word_dict[i]\n",
    "            pairs.append((w, cos_similairty[i]))\n",
    "        return pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('christ', 1.0),\n",
       " ('jesus', 0.7170907),\n",
       " ('gospel', 0.4621805),\n",
       " ('peter', 0.39412546),\n",
       " ('disciples', 0.3873747),\n",
       " ('noise', 0.28152165),\n",
       " ('asleep', 0.26372147),\n",
       " ('taught', 0.2422184),\n",
       " ('zarhites', 0.24168596),\n",
       " ('nobles', 0.23950878)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)\n",
    "cosinSim.get_synonym('christ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jesus', 1.0),\n",
       " ('christ', 0.7170907),\n",
       " ('gospel', 0.5360588),\n",
       " ('peter', 0.3603956),\n",
       " ('disciples', 0.3460646),\n",
       " ('church', 0.2755898),\n",
       " ('passed', 0.24744174),\n",
       " ('noise', 0.23768528),\n",
       " ('preach', 0.23454829),\n",
       " ('send', 0.2337867)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinSim.get_synonym('jesus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
