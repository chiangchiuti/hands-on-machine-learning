{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interstate-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple, NamedTuple\n",
    "from collections import OrderedDict\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data_path =  \"/Volumes/ExFAT/dataset/ml-1m/movies.dat\"\n",
    "user_data_path = \"/Volumes/ExFAT/dataset/ml-1m/users.dat\"\n",
    "ratings_data_path = \"/Volumes/ExFAT/dataset/ml-1m/ratings.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-small",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norwegian-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_columns = [\"gender\", \"age\", \"occupation\", \"zipCode\"] \n",
    "movie_columns = ['title', 'genres']\n",
    "user_df = pd.read_csv(user_data_path, sep=\"::\", header=None, engine=\"python\", names=user_columns)\n",
    "item_df = pd.read_csv(movie_data_path, sep=\"::\", header=None, engine=\"python\", names=movie_columns).drop('title', axis=1)\n",
    "\n",
    "rate_df = pd.read_csv(ratings_data_path, sep=\"::\", engine=\"python\", header=None, names=['userId', 'movieId', 'rating', 'timestamp']).drop('timestamp', axis=1) \n",
    "\n",
    "rate_df = rate_df.merge(user_df, left_on=['userId'], right_index=True, how='inner')\n",
    "rate_df = rate_df.merge(item_df, left_on=['movieId'], right_index=True, how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changing-sport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zipCode</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>32793</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>22903</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>95350</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>95825</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48073</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>10023</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>1193</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>14607</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>55421</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating gender  age  occupation zipCode genres\n",
       "0       1     1193       5      F    1          10   48067  Drama\n",
       "1       2     1193       5      M   56          16   70072  Drama\n",
       "2      12     1193       4      M   25          12   32793  Drama\n",
       "3      15     1193       4      M   25           7   22903  Drama\n",
       "4      17     1193       5      M   50           1   95350  Drama\n",
       "5      18     1193       4      F   18           3   95825  Drama\n",
       "6      19     1193       5      M    1          10   48073  Drama\n",
       "7      24     1193       5      F   25           7   10023  Drama\n",
       "8      28     1193       3      F   25           1   14607  Drama\n",
       "9      33     1193       5      M   45           3   55421  Drama"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "analyzed-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['rating']\n",
    "feature_cols = ['userId', 'movieId', 'gender', 'age', 'occupation', 'zipCode']\n",
    "X = rate_df[feature_cols].copy()\n",
    "Y = rate_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-mechanics",
   "metadata": {},
   "source": [
    "# Model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-melissa",
   "metadata": {},
   "source": [
    "## wide featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becoming-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aware-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cross_col_pairs: List[Tuple[str, str]]):\n",
    "        self.cross_col_pairs = cross_col_pairs\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        self.unique_columns_ = set()\n",
    "        for pair in self.cross_col_pairs:\n",
    "            self.unique_columns_.update(list(pair))\n",
    "        \n",
    "        self.crossed_colnamed_ = []\n",
    "        \n",
    "        for cols in self.cross_col_pairs:\n",
    "            cols = list(cols)\n",
    "            new_colname = \"_\".join(cols)\n",
    "            self.crossed_colnamed_.append(new_colname)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df: pd.DataFrame):\n",
    "            df_cross = df[self.unique_columns_].copy().astype(str)\n",
    "            \n",
    "            for cols in self.cross_col_pairs:\n",
    "                cols = list(cols)\n",
    "                new_colname = \"_\".join(cols)\n",
    "                df_cross[new_colname] = df_cross[cols[0]] + \\\n",
    "                    '-' + df_cross[cols[1]]\n",
    "            return df_cross[self.crossed_colnamed_]\n",
    "           \n",
    "class WideFeaturesGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wide_cols: List[str], cross_col_pairs=None):\n",
    "        self.wide_cols = wide_cols\n",
    "        self.cross_col_pairs = cross_col_pairs\n",
    "\n",
    "        self.cross_feature_transformer = CrossFeatures(self.cross_col_pairs)\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        self.cross_feature_transformer_ = self.cross_feature_transformer.fit(df)\n",
    "        self.crossed_colnamed_ = self.cross_feature_transformer_.crossed_colnamed_\n",
    "\n",
    "        df_wide = self._generate_wide_features(df)\n",
    "        self.wide_columns_ = df_wide.columns.tolist()\n",
    "        self.feature_dict_ = self._generate_global_feature_dict(df_wide)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        \n",
    "        df_wide = self._generate_wide_features(df)\n",
    "        \n",
    "        encoded = np.zeros([len(df_wide), len(self.wide_columns_)], dtype=np.long)\n",
    "        \n",
    "        for i , col in enumerate(self.wide_columns_):\n",
    "            encoded[:, i] = df_wide[col].apply(lambda x: self.feature_dict_[col +  '_' + str(x)])\n",
    "        return encoded.astype('int64')\n",
    "        \n",
    "\n",
    "    def _generate_global_feature_dict(self, df: pd.DataFrame):\n",
    "        columns = df.columns.tolist()\n",
    "        all_col_value = []\n",
    "        for col in columns:\n",
    "            unique_value = [col + '_' + str(x) for x in df[col].unique()]\n",
    "            all_col_value.extend(unique_value)\n",
    "        return {v: i + 1 for i, v in enumerate(all_col_value)}\n",
    "\n",
    "\n",
    "    \n",
    "    def _generate_wide_features(self, df):\n",
    "        df_cross = self.cross_feature_transformer_.transform(df)\n",
    "        df_wide = pd.concat([df[self.wide_cols], df_cross], axis=1)\n",
    "        return df_wide    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ideal-tutorial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_cols = ['gender', 'age', 'occupation', 'zipCode']\n",
    "crossed_cols = [('gender', 'age'), ('gender', 'occupation'), ('age', 'occupation')]\n",
    "wideGenerator = WideFeaturesGenerator(wide_cols, crossed_cols)\n",
    "x_wide = wideGenerator.fit_transform(X)\n",
    "x_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intelligent-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'age', 'occupation', 'zipCode', 'gender_age', 'gender_occupation', 'age_occupation']\n",
      "3659\n"
     ]
    }
   ],
   "source": [
    "print(wideGenerator.wide_columns_)\n",
    "print(len(wideGenerator.feature_dict_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-pound",
   "metadata": {},
   "source": [
    "## deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "level-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrapped-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_encode: List[str]):\n",
    "        self.columns_to_encode = columns_to_encode\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        df_ = df[self.columns_to_encode].copy()\n",
    "\n",
    "        for col in self.columns_to_encode:\n",
    "            df_[self.columns_to_encode] = df[self.columns_to_encode].astype(\n",
    "                'str')\n",
    "\n",
    "        unique_column_vals = {col: df_[col].unique()\n",
    "                              for col in self.columns_to_encode}\n",
    "\n",
    "        self.encoding_dict_ = dict()\n",
    "\n",
    "        for k, v in unique_column_vals.items():\n",
    "            self.encoding_dict_[k] = {val: idx for idx, val in enumerate(v)}\n",
    "            self.encoding_dict_[k]['unseen'] = len(self.encoding_dict_[k])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        try:\n",
    "            self.encoding_dict_\n",
    "        except AttributeError:\n",
    "            raise NotFittedError(\n",
    "                \"This LabelEncoder instance is not fitted yet. \"\n",
    "                \"Call 'fit' with appropriate arguments before using this LabelEncoder.\"\n",
    "            )\n",
    "        df_ = df.copy()\n",
    "        df_[self.columns_to_encode] = df_[self.columns_to_encode].astype('str')\n",
    "\n",
    "        for col, encoding_map in self.encoding_dict_.items():\n",
    "            original_value = [f for f in encoding_map.keys() if f != 'unseen']\n",
    "            df_[col] = np.where(df_[col].isin(\n",
    "                original_value), df_[col], 'unseen')\n",
    "            df_[col] = df_[col].apply(lambda x: encoding_map[x])\n",
    "        return df_\n",
    "\n",
    "\n",
    "class DeepFeaturesGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, embed_cols: List[str], continuous_cols: List[str]):\n",
    "        # category type\n",
    "        self.embed_cols = embed_cols\n",
    "        self.continuous_cols = continuous_cols \n",
    "\n",
    "        self.deep_cols = embed_cols + continuous_cols\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, y=None):\n",
    "        label_encoder = LabelEncoder(self.embed_cols)\n",
    "        self.label_encoder_ = label_encoder.fit(df)\n",
    "        self.embed_cols_unique_labels_ = {col: len(v) for col, v in self.label_encoder_.encoding_dict_.items()}\n",
    "\n",
    "        if self.continuous_cols:\n",
    "            df_continuous = df[self.continuous_cols].copy()\n",
    "            self.scalar_ = StandardScaler().fit(df_continuous.values.astype('float'))\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        df_embed = df[self.embed_cols].copy()\n",
    "        df_continuous = df[self.continuous_cols].copy()\n",
    "\n",
    "        df_embed = self.label_encoder_.transform(df_embed)\n",
    "        if self.continuous_cols:\n",
    "            df_continuous[self.continuous_cols] = self.scalar_.transform(df_continuous.values.astype('float'))\n",
    "            df_deep = pd.concat([df_embed, df_continuous], axis=1)[self.deep_cols]\n",
    "        else:\n",
    "            df_deep = df_embed[self.deep_cols]\n",
    "        return df_deep.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developed-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId 6040\n",
      "movieId 3706\n",
      "gender 2\n",
      "age 7\n",
      "occupation 21\n",
      "zipCode 3439\n"
     ]
    }
   ],
   "source": [
    "for col in X:\n",
    "    print(col, len(X[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unavailable-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_embed_dim_mapping = {\n",
    "    'userId': 50,\n",
    "    'movieId': 50,\n",
    "    'gender': 2,\n",
    "    'age':2,\n",
    "    'occupation': 5,\n",
    "    'zipCode': 20\n",
    "}\n",
    "category_cols = list(category_embed_dim_mapping.keys())\n",
    "continuous_cols = []\n",
    "deep_generator = DeepFeaturesGenerator(category_cols, continuous_cols)\n",
    "df_deep = X[category_cols + continuous_cols].copy()\n",
    "x_deep = deep_generator.fit_transform(X)\n",
    "x_deep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "olive-australian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 6041,\n",
       " 'movieId': 3707,\n",
       " 'gender': 3,\n",
       " 'age': 8,\n",
       " 'occupation': 22,\n",
       " 'zipCode': 3440}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_generator.embed_cols_unique_labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-heath",
   "metadata": {},
   "source": [
    "##  model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proper-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spectacular-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_wide, x_deep, target):\n",
    "        assert(x_wide.shape[0] == x_deep.shape[0] == target.shape[0])\n",
    "        self.x_wide = x_wide\n",
    "        self.x_deep = x_deep\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_wide[index], self.x_deep[index], self.target[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "individual-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 ms, sys: 3.19 ms, total: 8.69 ms\n",
      "Wall time: 7.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_y = Y.values.copy()\n",
    "target_y = (target_y > 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000209, 7), (1000209, 6), (1000209, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_deep = x_deep\n",
    "x_wide = x_wide\n",
    "x_wide.shape, x_deep.shape, target_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-asian",
   "metadata": {},
   "source": [
    "# Wide & deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "creative-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union, Tuple\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "binary-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide(nn.Module):\n",
    "    def __init__(self, wide_dim: int, predict_dim: int=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(wide_dim + 1, predict_dim, padding_idx=0) # reserve 1 dim for unseen cross feature\n",
    "        self.bias = nn.Parameter(torch.zeros(predict_dim))\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_normal_(self.linear.weight, a=math.sqrt(5))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.linear.weight)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "\n",
    "        # X [b_size, num_of_wide_features]\n",
    "        return self.linear(X.long()).sum(dim=1) + self.bias # [b_size, predict_dim]\n",
    "\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    EMBEDDING_LAYER_PREFIX = \"emb_layer\"\n",
    "    DENSE_LAYER_PREFIX = \"dense_layer\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 columns_index: Dict[str, int],\n",
    "                 embed_cols_info: List[Tuple[str, int, int]],  # (col_name, label_size, embeding_dim)\n",
    "                 continuous_cols: List[str],\n",
    "                 hidden_layer_neural: List[int],\n",
    "                 hidden_layer_dropout: List[float],\n",
    "                 embed_col_dropout: float=0.0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.columns_index = columns_index\n",
    "        self.embed_cols_info = embed_cols_info\n",
    "        self.continuous_cols = continuous_cols\n",
    "\n",
    "        self.embed_layers = self._create_embed_layers(embed_cols_info)\n",
    "        self.embed_dropout_layer = nn.Dropout(embed_col_dropout)\n",
    "\n",
    "        self.hidden_layer_neural = self._update_hidden_layer_neural(\n",
    "            hidden_layer_neural)\n",
    "\n",
    "        self.dense_layer = self._create_dense_layer(hidden_layer_dropout)\n",
    "        self.output_dim = hidden_layer_neural[-1]\n",
    "\n",
    "    def _create_embed_layers(self, embed_cols_info: List[Tuple[str, int, int]]):\n",
    "        return nn.ModuleDict({self.EMBEDDING_LAYER_PREFIX + '_' + col_name.replace(\".\", '_'): nn.Embedding(num_label, dim) for col_name, num_label, dim in embed_cols_info})\n",
    "\n",
    "    def _create_dense_layer(self, hidden_layer_dropout):\n",
    "        dense_dequential = nn.Sequential()\n",
    "        for i in range(1, len(self.hidden_layer_neural)):\n",
    "            dense_dequential.add_module(\n",
    "                \"{}_{}\".format(self.DENSE_LAYER_PREFIX, i - 1),\n",
    "                self._create_dense_component(\n",
    "                    self.hidden_layer_neural[i-1], self.hidden_layer_neural[i], hidden_layer_dropout[i-1], True)\n",
    "            )\n",
    "        return dense_dequential\n",
    "\n",
    "    def _update_hidden_layer_neural(self, hidden_layer_neurals: List[int]):\n",
    "        embed_dim = sum([embed[2] for embed in self.embed_cols_info])\n",
    "        continuous_dim = len(self.continuous_cols)\n",
    "        return [embed_dim + continuous_dim] + hidden_layer_neurals\n",
    "\n",
    "    def _create_dense_component(self, input_dim: int, output_dim: int, dropout_ratio: float=0.0, batch_norm=False):\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(output_dim))\n",
    "        layers.append(nn.Dropout(dropout_ratio))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __get_embeding_layer(self, embed_col):\n",
    "        embed_col = self.EMBEDDING_LAYER_PREFIX + '_' + embed_col.replace('.', '_')\n",
    "        return self.embed_layers[embed_col]\n",
    "        \n",
    "    def forward(self, deep_input_x: torch.Tensor):\n",
    "        embed_x = [\n",
    "           self.__get_embeding_layer(col)(deep_input_x[:, self.columns_index[col]].long())\n",
    "            for col, _, _ in self.embed_cols_info\n",
    "        ]\n",
    "\n",
    "        embed_x = torch.cat(embed_x, 1)\n",
    "\n",
    "        continuous_cols_idx = [self.columns_index[col]\n",
    "                              for col in self.continuous_cols]\n",
    "        continuous_x = deep_input_x[:, continuous_cols_idx].float()\n",
    "\n",
    "        x = torch.cat([embed_x, continuous_x], dim=1)\n",
    "        return self.dense_layer(x)  # [b_size, hidden_layer_last_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "completed-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, wide: nn.Module, deep: nn.Module):\n",
    "        super().__init__()\n",
    "        \n",
    "        deep = nn.Sequential(\n",
    "            deep,\n",
    "            nn.Linear(deep.output_dim, 1)\n",
    "        )\n",
    "        self.wide_deep = nn.ModuleDict({\n",
    "            \"wide\": wide,\n",
    "            \"deep\": deep\n",
    "        })\n",
    "\n",
    "    def forward(self, x_wide: torch.Tensor, x_deep: torch.Tensor):\n",
    "        wide_out = self.wide_deep['wide'](x_wide)  # [b_size, 1]\n",
    "        deep_out = self.wide_deep['deep'](x_deep)  # [b_size, 1]\n",
    "        out = wide_out + deep_out\n",
    "        return out.view(-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, x_wide: torch.Tensor, x_deep: torch.Tensor, threshold: int=0.5):\n",
    "        logistic = self.predict_probs(x_wide, x_deep)\n",
    "        return (logistic > threshold).int()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_probs(self, x_wide: torch.Tensor, x_deep: torch.Tensor):\n",
    "        out = self.forward(x_wide, x_deep)\n",
    "        return torch.sigmoid(out.view(-1).float())\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-prevention",
   "metadata": {},
   "source": [
    "# Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prostate-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "electoral-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000209, 7), (1000209, 6), (1000209, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_wide.shape, x_deep.shape, target_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "injured-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800167, 7), (800167, 6), (800167, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_wide, test_x_wide, train_x_deep, test_x_deep, train_y, test_y = train_test_split(x_wide, x_deep, target_y, test_size=0.2)\n",
    "train_x_wide.shape, train_x_deep.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pleasant-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_wide = torch.from_numpy(test_x_wide)\n",
    "test_x_deep =  torch.from_numpy(test_x_deep)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-traveler",
   "metadata": {},
   "source": [
    "## evaluation func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "norwegian-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cal_metrics(model, X: torch.Tensor, Y: torch.Tensor)-> Dict[str, float]:\n",
    "    model.eval()\n",
    "    x_wide, x_deep = X\n",
    "\n",
    "    y_target = Y.detach().numpy()\n",
    "    y_prob = model.predict_probs(x_wide, x_deep).detach().numpy()\n",
    "    y_pre_label = (y_prob > 0.5).astype(int)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y_target, y_pre_label).ravel()\n",
    "        \n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_target, y_prob)\n",
    "\n",
    "    model.train()\n",
    "    return {'prec': precision, 'recall': recall, 'acc': acc, 'auc': auc}\n",
    "    \n",
    "def validation_step(model: nn.Module, X: torch.Tensor, Y: torch.Tensor, loss_fn)-> Tuple[float, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_wide, x_deep = X\n",
    "        y_pre = model(x_wide, x_deep)\n",
    "        loss = loss_fn(y_pre.view(-1).float(), Y.float().view(-1))  # mean loss\n",
    "    \n",
    "    model.train()\n",
    "    return loss.item(), cal_metrics(model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-credits",
   "metadata": {},
   "source": [
    "## training with AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approved-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = WideDeepDataset(train_x_wide, train_x_deep, train_y)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pressed-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols_info = [(col, deep_generator.embed_cols_unique_labels_[col], embed_dim)for col, embed_dim in category_embed_dim_mapping.items()]\n",
    "deep_column_idx = {col: i for i, col in enumerate(deep_generator.deep_cols)}\n",
    "hidden_layers = [100, 64, 32]\n",
    "drop_out = [0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "terminal-injection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 6041, 50),\n",
       " ('movieId', 3707, 50),\n",
       " ('gender', 3, 2),\n",
       " ('age', 8, 2),\n",
       " ('occupation', 22, 5),\n",
       " ('zipCode', 3440, 20)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_cols_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "corresponding-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=np.unique(x_wide).shape[0], predict_dim=1)\n",
    "deep = Deep(deep_column_idx, embed_cols_info, continuous_cols, hidden_layers, drop_out, 0.2)\n",
    "wide_deep = WideDeep(wide, deep)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "general_optimizer = torch.optim.AdamW(wide_deep.parameters(),  lr=0.001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "removed-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2668/2668 [02:04<00:00, 21.36it/s, metrics={'prec': 0.7153, 'recall': 0.7992, 'acc': 0.7019, 'auc': 0.7586}, train_loss=0.593, valid_loss=0.574]\n",
      "100%|██████████| 2668/2668 [01:41<00:00, 26.27it/s, metrics={'prec': 0.737, 'recall': 0.8025, 'acc': 0.722, 'auc': 0.7848}, train_loss=0.553, valid_loss=0.548] \n",
      "100%|██████████| 2668/2668 [01:48<00:00, 24.56it/s, metrics={'prec': 0.7297, 'recall': 0.8277, 'acc': 0.7248, 'auc': 0.79}, train_loss=0.542, valid_loss=0.542]  \n",
      "100%|██████████| 2668/2668 [02:12<00:00, 20.16it/s, metrics={'prec': 0.7334, 'recall': 0.8221, 'acc': 0.7261, 'auc': 0.7922}, train_loss=0.536, valid_loss=0.539]\n",
      "100%|██████████| 2668/2668 [02:27<00:00, 18.14it/s, metrics={'prec': 0.7357, 'recall': 0.8191, 'acc': 0.727, 'auc': 0.793}, train_loss=0.53, valid_loss=0.538]   \n",
      "100%|██████████| 2668/2668 [01:51<00:00, 23.89it/s, metrics={'prec': 0.732, 'recall': 0.8274, 'acc': 0.7268, 'auc': 0.7935}, train_loss=0.528, valid_loss=0.538] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_interval = 500\n",
    "epoch = 6\n",
    "count = 0\n",
    "for epoch_i in range(epoch):\n",
    "    total_loss = 0\n",
    "   \n",
    "    tk0 = tqdm.tqdm(train_data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (x_w, x_d, target_y) in enumerate(tk0):\n",
    "        wide_deep.train()\n",
    "        y = wide_deep(x_w, x_d)\n",
    "        loss = criterion(y.view(-1), target_y.float().view(-1))\n",
    "        \n",
    "        wide_deep.zero_grad()\n",
    "        loss.backward()\n",
    "        general_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        \n",
    "        if count % log_interval == 0:\n",
    "            valid_loss, score = validation_step(wide_deep, (test_x_wide, test_x_deep), test_y, criterion)\n",
    "            tk0.set_postfix(train_loss=total_loss/log_interval, valid_loss=valid_loss, metrics={k: np.round(v, 4) for k, v in score.items()})\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "modular-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prec': 0.7346040827749187,\n",
       " 'recall': 0.8222365306974802,\n",
       " 'acc': 0.7272072864698413,\n",
       " 'auc': 0.793531092886336}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_metrics(wide_deep, (test_x_wide, test_x_deep), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-efficiency",
   "metadata": {},
   "source": [
    "# Online Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-terror",
   "metadata": {},
   "source": [
    "## Feature Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "theoretical-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProvider:\n",
    "    def __init__(self, df: pd.DataFrame, index_col_name):\n",
    "        self.df = df.copy()\n",
    "        self.index_col_name = index_col_name\n",
    "    \n",
    "    def query_features(self, index: List[int], cols: List[str])-> pd.DataFrame:\n",
    "        df = self.df\n",
    "        return df[df[self.index_col_name].isin(index)][cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "handled-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(user_data_path, sep=\"::\", header=None, engine=\"python\", names=user_columns)\n",
    "user_df = user_df.reset_index().rename(columns={'index': 'userId'})\n",
    "\n",
    "item_df = pd.read_csv(movie_data_path, sep=\"::\", header=None, engine=\"python\", names=movie_columns)\n",
    "item_df = item_df.reset_index().rename(columns={'index': 'movieId'})\n",
    "\n",
    "user_feature_provider = FeatureProvider(user_df, 'userId')\n",
    "item_feature_provider = FeatureProvider(item_df, 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "metallic-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age gender\n",
       "4   25      M\n",
       "9   35      F"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature_provider.query_features(index=[5, 10], cols=['age', 'gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-slovak",
   "metadata": {},
   "source": [
    "## Embedding Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "environmental-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProvider:\n",
    "    def __init__(self, embedding_dict: nn.ModuleDict, prefix='emb_layer'):\n",
    "        self.prefix= prefix\n",
    "        self.embedding_dict = embedding_dict\n",
    "        \n",
    "    def query_embedding(self, batch_labels:np.array, label_order:List[str])-> torch.Tensor:\n",
    "        # batch_label: [b_size, num of labels]\n",
    "        label_order = [self.prefix + '_' + str(label) for label in label_order]\n",
    "        \n",
    "        batch_labels = torch.from_numpy(batch_labels).long()\n",
    "        embed_X = [\n",
    "           self.embedding_dict[label](batch_labels[:, idx]) for idx, label in enumerate(label_order)\n",
    "        ]\n",
    "        return torch.cat(embed_X, 1) # [b_size, sum of all embedding dims]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "marine-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_provider = EmbeddingProvider(wide_deep.wide_deep['deep'][0].embed_layers, 'emb_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "manufactured-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (emb_layer_age): Embedding(8, 2)\n",
       "  (emb_layer_gender): Embedding(3, 2)\n",
       "  (emb_layer_movieId): Embedding(3707, 50)\n",
       "  (emb_layer_occupation): Embedding(22, 5)\n",
       "  (emb_layer_userId): Embedding(6041, 50)\n",
       "  (emb_layer_zipCode): Embedding(3440, 20)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_provider.embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-restriction",
   "metadata": {},
   "source": [
    "## Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "interested-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepOnlineRanker(nn.Module):\n",
    "    def __init__(self, offlineWideDeep: nn.Module):\n",
    "        super().__init__()\n",
    "        self.deep_dense = self._fetch_deep_dense(offlineWideDeep)\n",
    "        self.wide_part = offlineWideDeep.wide_deep['wide']\n",
    "    \n",
    "    def _fetch_deep_dense(self, offlineWideDeep):\n",
    "        deep_dense = []\n",
    "        for dense_layer in offlineWideDeep.wide_deep['deep'][0].dense_layer:\n",
    "            new_layer = nn.Sequential(*[d for d in dense_layer if not isinstance(d, nn.Dropout)])\n",
    "            deep_dense.append(new_layer)\n",
    "        deep_dense.append(offlineWideDeep.wide_deep['deep'][1])\n",
    "\n",
    "        return nn.Sequential(*deep_dense)\n",
    "    \n",
    "\n",
    "    def forward(self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor):\n",
    "        wide_output = self.wide_part(wide_x.long())\n",
    "        deep_output = self.deep_dense(deep_x_embedding)\n",
    "        \n",
    "        \n",
    "        return (wide_output + deep_output).view(-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def scoring(self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor):\n",
    "        self.eval()\n",
    "        return self.forward(wide_x, deep_x_embedding).view(-1).float()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_probs(self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor):\n",
    "        self.eval()\n",
    "        out = self.forward(wide_x, deep_x_embedding)\n",
    "        return torch.sigmoid(out.view(-1)).float()\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "terminal-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = WideDeepOnlineRanker(wide_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-darkness",
   "metadata": {},
   "source": [
    "## Ranking Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "several-worse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1496, 1643, 2040, 3515, 1711, 2822, 1959,   35, 3023, 2651, 2776,\n",
       "       2095,  245, 3285, 2926, 2952, 3890, 3208, 2841, 3221,  256, 1381,\n",
       "       1937, 1885, 3132,  729,  190, 1407, 1894, 3947])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 1000\n",
    "match_itemId = np.random.choice(item_df['movieId'], 30)\n",
    "match_itemId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "based-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_user_features_cols = ['userId', 'gender', 'age', 'occupation', 'zipCode']\n",
    "query_item_features_cols = ['movieId']\n",
    "\n",
    "user_primitive_feature = user_feature_provider.query_features([user_id], query_user_features_cols)\n",
    "item_primitive_feature = item_feature_provider.query_features(list(match_itemId), query_item_features_cols)\n",
    "\n",
    "user_primitive_feature['join'] = 1\n",
    "item_primitive_feature['join'] = 1\n",
    "primitive_features = user_primitive_feature.merge(item_primitive_feature, on = ['join']).drop('join', axis=1)\n",
    "processed_wide_features = wideGenerator.transform(primitive_features)\n",
    "processed_deep_features = deep_generator.transform(primitive_features)\n",
    "\n",
    "\n",
    "embedding_label_order = ['userId', 'movieId', 'gender', 'age', 'occupation', 'zipCode']\n",
    "embedding_features = embedding_provider.query_embedding(processed_deep_features, embedding_label_order)\n",
    "\n",
    "processed_wide_features = torch.from_numpy(processed_wide_features).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "resident-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId:3023 \t 'My Best Girl (1927)' \t score:0.826\n",
      "movieId:1937 \t 'Going My Way (1944)' \t score:0.815\n",
      "movieId:1711 \t 'Midnight in the Garden of Good and Evil (1997)' \t score:0.788\n",
      "movieId:2776 \t 'Marcello Mastroianni: I Remember Yes, I Remember (1997)' \t score:0.774\n",
      "movieId:1381 \t 'Grease 2 (1982)' \t score:0.773\n",
      "movieId:245 \t 'Glass Shield, The (1994)' \t score:0.769\n",
      "movieId:1643 \t 'Mrs. Brown (Her Majesty, Mrs. Brown) (1997)' \t score:0.763\n",
      "movieId:3208 \t 'Loaded Weapon 1 (1993)' \t score:0.735\n",
      "movieId:1885 \t 'Opposite of Sex, The (1998)' \t score:0.717\n",
      "movieId:3221 \t 'Draughtsman's Contract, The (1982)' \t score:0.703\n",
      "movieId:3285 \t 'Beach, The (2000)' \t score:0.697\n",
      "movieId:1959 \t 'Out of Africa (1985)' \t score:0.644\n",
      "movieId:729 \t 'Institute Benjamenta, or This Dream People Call Human Life (1995)' \t score:0.642\n",
      "movieId:3890 \t 'Back Stage (2000)' \t score:0.637\n",
      "movieId:1496 \t 'Anna Karenina (1997)' \t score:0.621\n",
      "movieId:256 \t 'Junior (1994)' \t score:0.59\n",
      "movieId:2651 \t 'Frankenstein Meets the Wolf Man (1943)' \t score:0.583\n",
      "movieId:3947 \t 'Get Carter (1971)' \t score:0.581\n",
      "movieId:1407 \t 'Scream (1996)' \t score:0.573\n",
      "movieId:1894 \t 'Six Days Seven Nights (1998)' \t score:0.56\n",
      "movieId:2841 \t 'Stir of Echoes (1999)' \t score:0.533\n",
      "movieId:2926 \t 'Hairspray (1988)' \t score:0.507\n",
      "movieId:3132 \t 'Daddy Long Legs (1919)' \t score:0.475\n",
      "movieId:2095 \t 'Shaggy D.A., The (1976)' \t score:0.421\n",
      "movieId:190 \t 'Safe (1995)' \t score:0.415\n",
      "movieId:2040 \t 'Computer Wore Tennis Shoes, The (1970)' \t score:0.404\n",
      "movieId:2952 \t 'Hard 8 (a.k.a. Sydney, a.k.a. Hard Eight) (1996)' \t score:0.402\n",
      "movieId:3515 \t 'Me Myself I (2000)' \t score:0.372\n",
      "movieId:35 \t 'Carrington (1995)' \t score:0.371\n",
      "movieId:2822 \t 'Medicine Man (1992)' \t score:0.115\n"
     ]
    }
   ],
   "source": [
    "scores = ranker.predict_probs(processed_wide_features, embedding_features)\n",
    "\n",
    "sorted_index = np.argsort(scores.detach().numpy(), axis=0)[::-1]\n",
    "for idx in sorted_index:\n",
    "    movieId = match_itemId[idx]\n",
    "    title = item_feature_provider.query_features([movieId], ['title'])['title'].item()\n",
    "    \n",
    "    print(\"movieId:{} \\t '{}' \\t score:{}\".format(movieId, title,  round(scores[idx].item(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-exposure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-knowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
